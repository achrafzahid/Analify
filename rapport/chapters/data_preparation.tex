\chapter{Préparation et Transformation des Données}

\section{Introduction}

La construction d'une plateforme d'analytique robuste nécessite des données de qualité, structurées et cohérentes. Ce chapitre présente le processus complet de transformation du fichier CSV brut (\texttt{superstore\_dataset.csv}) en un ensemble de tables relationnelles exploitables dans notre base de données PostgreSQL.

Le dataset initial contient des données de ventes d'une chaîne de magasins, mais présente plusieurs défis typiques des données réelles : redondance, absence d'identifiants uniques, données non normalisées, et nécessité de créer des entités complémentaires (utilisateurs, rôles, inventaires).

\section{Architecture de Transformation}

\subsection{Outils et Technologies}

La transformation des données a été réalisée avec les technologies suivantes :

\begin{itemize}
    \item \textbf{Python 3.x} : Langage de programmation principal
    \item \textbf{Pandas} : Manipulation et transformation des données
    \item \textbf{NumPy} : Génération de données numériques aléatoires
    \item \textbf{Faker} : Génération de données synthétiques (noms, emails, dates)
    \item \textbf{SQLAlchemy} : Connexion et injection dans PostgreSQL
    \item \textbf{Jupyter Notebook} : Environnement de développement interactif
\end{itemize}

\subsection{Connexion à la Base de Données}

La connexion à PostgreSQL a été établie via SQLAlchemy avec les paramètres suivants :

\begin{lstlisting}[language=Python, caption=Configuration de la connexion PostgreSQL]
from sqlalchemy import create_engine

engine = create_engine(
    'postgresql+psycopg2://postgres:Admin@localhost:5432/analify'
)
\end{lstlisting}

\section{Normalisation et Création des Identifiants}

\subsection{Principe de Normalisation}

Le dataset original contient des colonnes textuelles redondantes (région, catégorie, produit, etc.). Pour respecter les principes de normalisation des bases de données relationnelles, nous avons :

\begin{enumerate}
    \item Extrait chaque dimension unique
    \item Créé des tables de référence avec identifiants auto-incrémentés
    \item Fusionné les identifiants dans le dataset principal
\end{enumerate}

\subsection{Création des Identifiants Géographiques}

\subsubsection{Régions}

Extraction des régions uniques et création des identifiants :

\begin{lstlisting}[language=Python, caption=Normalisation des régions]
df["region"] = df["region"].str.strip().str.lower()

df_region = (
    df[["region"]]
    .drop_duplicates()
    .reset_index(drop=True)
)

df_region["region_id"] = df_region.index + 1
df = df.merge(df_region, on="region", how="left")
\end{lstlisting}

\textbf{Résultat} : Création de la table \texttt{region} avec 4 régions distinctes (East, West, Central, South).

\subsubsection{États et Villes}

Le même processus a été appliqué pour les états et les villes :

\begin{lstlisting}[language=Python, caption=Normalisation des états]
df["state"] = df["state"].str.strip().str.lower()

df_state = (
    df[["state"]]
    .drop_duplicates()
    .reset_index(drop=True)
)

df_state["state_id"] = df_state.index + 1
df = df.merge(df_state, on="state", how="left")
\end{lstlisting}

Pour les villes, nous avons utilisé le code postal (\texttt{zip}) comme identifiant unique :

\begin{lstlisting}[language=Python, caption=Normalisation des villes]
city_df = df[["zip", "city", "state_id"]]
city_df = (
    city_df
    .groupby("city", as_index=False)
    .agg({"zip": lambda x: x.mode().iloc[0]})
)

city_df = city_df.rename(columns={"zip": "city_id"})
\end{lstlisting}

\subsection{Création des Identifiants Produits}

\subsubsection{Catégories et Sous-catégories}

Les produits sont organisés hiérarchiquement en catégories et sous-catégories :

\begin{lstlisting}[language=Python, caption=Hiérarchie des produits]
# Categories
df["category"] = df["category"].str.strip().str.lower()
df_category = (
    df[["category"]]
    .drop_duplicates()
    .reset_index(drop=True)
)
df_category["category_id"] = df_category.index + 1

# Subcategories
df["subcategory"] = df["subcategory"].str.strip().str.lower()
df_subcategory = (
    df[["subcategory", "category_id"]]
    .drop_duplicates()
    .reset_index(drop=True)
)
df_subcategory["subcategory_id"] = df_subcategory.index + 1
\end{lstlisting}

\textbf{Résultat} : 3 catégories principales (Furniture, Technology, Office Supplies) et 17 sous-catégories.

\subsubsection{Produits}

Extraction des produits uniques avec association à leur sous-catégorie :

\begin{lstlisting}[language=Python, caption=Table des produits]
df["product_name"] = df["product_name"].str.strip().str.lower()

products_df = (
    df[["product_id", "product_name", "subcategory_id"]]
    .drop_duplicates(subset=['product_name'])
    .reset_index(drop=True)
)
\end{lstlisting}

\textbf{Résultat} : 1852 produits distincts catalogués.

\subsection{Magasins (Stores)}

Les magasins ont été créés à partir des villes uniques. Chaque ville correspond à un magasin :

\begin{lstlisting}[language=Python, caption=Création des magasins]
df["store_id"] = (
    df["city"]
    .astype("category")
    .cat.codes + 1
)

store_df = pd.DataFrame({
    'store_id': df["store_id"].unique(),
    'city_id': city_df['city_id'].unique()
})
\end{lstlisting}

\textbf{Résultat} : 531 magasins répartis dans différentes villes américaines.

\section{Génération des Utilisateurs et Rôles}

\subsection{Stratégie de Génération}

Analify nécessite quatre types d'utilisateurs distincts :
\begin{itemize}
    \item \textbf{Caissiers} : Employés de première ligne
    \item \textbf{Admin Store} : Gestionnaires de magasin
    \item \textbf{Investisseurs} : Participants au système de bidding
    \item \textbf{Admin Global} : Administrateur système unique
\end{itemize}

\subsection{Génération de la Base Utilisateurs}

Utilisation de la bibliothèque Faker pour créer 2000 utilisateurs synthétiques :

\begin{lstlisting}[language=Python, caption=Génération des utilisateurs avec Faker]
from faker import Faker

fake = Faker()
n = 2000

ids = []
names = []
emails = []
passwords = []
dates_of_birth = []

for i in range(1, n+1):
    ids.append(i)
    name = fake.name().replace(" ", "").lower()
    names.append(name)
    emails.append(f"{name}{i}@gmail.com")
    passwords.append("javajee123")
    dates_of_birth.append(
        fake.date_of_birth(minimum_age=18, maximum_age=70)
    )

users_df = pd.DataFrame({
    'id': ids,
    'user_name': names,
    'mail': emails,
    'password': passwords,
    'date_of_birth': dates_of_birth
})
\end{lstlisting}

\subsection{Création des Caissiers}

Les caissiers ont été créés en association avec les commandes existantes. Chaque zone géographique (identifiée par code postal) se voit attribuer 2 caissiers :

\begin{lstlisting}[language=Python, caption=Attribution des caissiers par zone]
orders_unique = (
    df[["order_id", "zip"]]
    .drop_duplicates()
    .sort_values(["zip", "order_id"])
    .reset_index(drop=True)
)

orders_unique["zip_index"] = (
    orders_unique["zip"]
    .astype("category")
    .cat.codes
)

NB_CAISSIERS_PAR_ZIP = 2

orders_unique["caissier_id"] = (
    orders_unique.groupby("zip")
    .cumcount() % NB_CAISSIERS_PAR_ZIP
) + 1 + orders_unique["zip_index"] * NB_CAISSIERS_PAR_ZIP
\end{lstlisting}

Ajout des informations spécifiques aux caissiers :

\begin{lstlisting}[language=Python, caption=Données complémentaires des caissiers]
from datetime import datetime, date

start = date(2010, 1, 1)
end = date.today()

caissier_df['date_started'] = [
    fake.date_between(start_date=start, end_date=end) 
    for _ in range(len(caissier_df))
]

# Salaire proportionnel a l'anciennete
today = datetime.today()
caissier_df['salaire'] = 2000 + 100 * caissier_df['date_started'].apply(
    lambda x: today.year - x.year
)
\end{lstlisting}

\textbf{Résultat} : 1286 caissiers avec ancienneté et salaire variables (2000-3400\$).

\subsection{Création des Administrateurs de Magasin}

Sélection aléatoire de 531 utilisateurs (un par magasin) :

\begin{lstlisting}[language=Python, caption=Administrateurs de magasin]
ids_caissier = set(caissier_df['ID'])
df_disponible = users_df[~users_df['ID'].isin(ids_caissier)]

admin_store_df = df_disponible.sample(n=531, random_state=42)

# Association magasin
admin_store_df["store_id"] = store_df["store_id"].values

# Salaire superieur aux caissiers
admin_store_df['salaire'] = 10000 + 100 * admin_store_df['date_started'].apply(
    lambda x: today.year - x.year
)
\end{lstlisting}

\textbf{Résultat} : 531 administrateurs avec salaire moyen 11500\$.

\subsection{Création des Investisseurs}

Sélection de 182 investisseurs parmi les utilisateurs restants :

\begin{lstlisting}[language=Python, caption=Création des investisseurs]
ids_utilises = set(caissier_df['ID']).union(set(admin_store_df['ID']))
df_disponible = users_df[~users_df['ID'].isin(ids_utilises)]

investor_df = df_disponible.sample(n=182, random_state=42)

# Noms remplaces par noms de fabricants (manufacturers)
mask = users_df["ID"].isin(investor_df["ID"])
users_df.loc[mask, "user_name"] = np.random.choice(
    df["manufactory"].unique(),
    size=mask.sum(),
    replace=True
)
\end{lstlisting}

Les investisseurs sont ensuite associés aléatoirement aux produits pour le système de bidding.

\subsection{Administrateur Global}

Un seul administrateur global a été créé :

\begin{lstlisting}[language=Python, caption=Admin global unique]
adminG_df = df_disponible.sample(n=1, random_state=42)
\end{lstlisting}

\section{Création des Données Transactionnelles}

\subsection{Commandes (Orders)}

Extraction des commandes uniques du dataset :

\begin{lstlisting}[language=Python, caption=Table des commandes]
orders_df = df[["order_id", "order_date", "ship_date", "caissier_id"]]
orders_df["order_id"] = orders_df["order_id"].str.slice(start=8)
orders_df = orders_df.rename(columns={"caissier_id": "user_id"})
orders_df = orders_df.drop_duplicates()

# Conversion en dates
orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])
orders_df['ship_date'] = pd.to_datetime(orders_df['ship_date'])
\end{lstlisting}

\textbf{Résultat} : 9994 commandes avec dates de commande et d'expédition.

\subsection{Articles de Commande (Order Items)}

Création de la table de jointure entre commandes et produits :

\begin{lstlisting}[language=Python, caption=Lignes de commande]
orderItems_df = (
    df[["order_id", "product_id", "sales", "quantity", "discount"]]
    .reset_index(drop=True)
)

orderItems_df["item_id"] = orderItems_df.index + 1
orderItems_df = orderItems_df.rename(columns={"sales": "price"})
orderItems_df["order_id"] = orderItems_df["order_id"].str.slice(start=8)
\end{lstlisting}

\textbf{Résultat} : 9994 lignes de commande avec prix, quantité et réduction.

\subsection{Inventaire (Inventory)}

L'inventaire a été créé pour toutes les combinaisons possibles magasin × produit :

\begin{lstlisting}[language=Python, caption=Génération complète de l'inventaire]
# Creer la grille complete
all_stores = df['store_id'].unique()
all_products = products_df['product_id'].unique()

grid = pd.MultiIndex.from_product(
    [all_stores, all_products], 
    names=['store_id', 'product_id']
).to_frame(index=False)

# Fusion avec inventaire existant
df_complete = pd.merge(
    grid, 
    inventory.drop(columns=['inventory_id']), 
    on=['store_id', 'product_id'], 
    how='left'
)

# Quantites aleatoires (distribution Beta)
df_complete['quantity'] = (
    np.random.beta(a=0.5, b=4, size=len(df)) * 210
).astype(int)

df_complete['quantity'] = df_complete['quantity'].fillna(0).astype(int)
df_complete['id'] = df_complete.index + 1
\end{lstlisting}

\textbf{Résultat} : 983,572 entrées d'inventaire (531 magasins × 1852 produits).

La distribution Beta ($\alpha=0.5, \beta=4$) a été choisie pour simuler un inventaire réaliste avec :
\begin{itemize}
    \item Majorité de produits avec stock faible (0-50 unités)
    \item Quelques produits populaires avec stock élevé (150-210 unités)
\end{itemize}

\section{Création du Système de Bidding}

\subsection{Hiérarchie du Bidding}

Le système de bidding suit une structure hiérarchique à 4 niveaux :

\begin{center}
\textbf{Catégorie → Rang → Face → Section}
\end{center}

\subsection{Rangs (Ranks)}

Création de 9 rangs (3 par catégorie) :

\begin{lstlisting}[language=Python, caption=Création des rangs]
n_rangs = 9

rang_ids = []
rang_names = []
descriptions = []
category_ids = []

for rang_id in range(1, n_rangs + 1):
    category_id = ((rang_id - 1) // 3) + 1
    
    rang_ids.append(rang_id)
    rang_names.append(f"rang_{rang_id}")
    descriptions.append(
        f"This rank represents position level {rang_id} "
        f"within the system hierarchy."
    )
    category_ids.append(category_id)

rang_df = pd.DataFrame({
    "rang_id": rang_ids,
    "rang_name": rang_names,
    "description": descriptions,
    "category_id": category_ids
})
\end{lstlisting}

\subsection{Faces}

Création de 18 faces (2 par rang) :

\begin{lstlisting}[language=Python, caption=Création des faces]
n_faces = 18

face_ids = list(range(1, n_faces + 1))
face_names = [f"face_{i}" for i in face_ids]
rang_ids = []

for face_id in face_ids:
    rang_id = (face_id + 1) // 2
    rang_ids.append(rang_id)
    
face_df = pd.DataFrame({
    "face_id": face_ids,
    "face_name": face_names,
    "rang_id": rang_ids
})
\end{lstlisting}

\subsection{Sections}

Création de 360 sections (20 par face) avec prix de base aléatoires :

\begin{lstlisting}[language=Python, caption=Création des sections avec prix]
from datetime import date, timedelta
import random

n_faces = 18
sections_per_face = 20
date_delai_value = date.today() + timedelta(days=30)

section_id = 1

for face_id in range(1, n_faces + 1):
    for position in range(1, sections_per_face + 1):
        # Prix multiple de 100 entre 2000 et 6000
        price = random.randrange(2000, 6001, 100)
        
        section_ids.append(section_id)
        section_names.append(f"section_{section_id}")
        face_ids.append(face_id)
        
        base_prices.append(price)
        current_prices.append(price)
        statuses.append("CLOSE")
        date_delais.append(date_delai_value)
        
        section_id += 1

section_df = pd.DataFrame({
    "section_id": section_ids,
    "section_name": section_names,
    "face_id": face_ids,
    "base_price": base_prices,
    "current_price": current_prices,
    "date_delai": date_delais,
    "status": statuses
})
\end{lstlisting}

\textbf{Résultat} : 360 sections avec prix de base variant de 2000\$ à 6000\$.

\section{Injection dans PostgreSQL}

\subsection{Méthode d'Injection}

Toutes les tables ont été injectées via la méthode \texttt{to\_sql()} de Pandas :

\begin{lstlisting}[language=Python, caption=Injection type dans PostgreSQL]
table_df.to_sql(
    "nom_table",
    engine,
    if_exists="append",
    index=False
)
\end{lstlisting}

\subsection{Ordre d'Injection}

L'ordre d'injection respecte les contraintes de clés étrangères :

\begin{enumerate}
    \item Tables de base : \texttt{region}, \texttt{state}, \texttt{city}, \texttt{category}, \texttt{subcategory}
    \item Utilisateurs : \texttt{user}
    \item Magasins : \texttt{store}
    \item Rôles : \texttt{caissier}, \texttt{admin\_store}, \texttt{investor}, \texttt{adming}
    \item Produits : \texttt{product} (avec \texttt{ID\_investisseur})
    \item Transactions : \texttt{orders}, \texttt{order\_items}
    \item Inventaire : \texttt{inventory}
    \item Bidding : \texttt{rang}, \texttt{face}, \texttt{section}
\end{enumerate}

\subsection{Volumétrie Finale}

Le tableau suivant résume les volumes de données injectés :

\begin{center}
\begin{tabular}{|l|r|l|}
\hline
\textbf{Table} & \textbf{Lignes} & \textbf{Description} \\
\hline
user & 2000 & Utilisateurs (tous rôles) \\
region & 4 & Régions géographiques \\
state & 49 & États américains \\
city & 531 & Villes \\
store & 531 & Magasins \\
category & 3 & Catégories principales \\
subcategory & 17 & Sous-catégories \\
product & 1852 & Produits uniques \\
caissier & 1286 & Caissiers \\
admin\_store & 531 & Administrateurs magasin \\
investor & 182 & Investisseurs \\
adming & 1 & Administrateur global \\
orders & 9994 & Commandes \\
order\_items & 9994 & Lignes de commande \\
inventory & 983572 & Stocks (store × product) \\
rang & 9 & Rangs de bidding \\
face & 18 & Faces de bidding \\
section & 360 & Sections bidding \\
\hline
\textbf{Total} & \textbf{1 009 905} & \textbf{Lignes totales} \\
\hline
\end{tabular}
\end{center}

\section{Validation et Vérifications}

\subsection{Contrôles de Cohérence}

Plusieurs vérifications ont été effectuées pour garantir la qualité des données :

\begin{itemize}
    \item \textbf{Unicité des identifiants} : Aucun doublon dans les clés primaires
    \item \textbf{Intégrité référentielle} : Toutes les clés étrangères référencent des entités existantes
    \item \textbf{Cohérence des rôles} : Aucun utilisateur n'a plusieurs rôles simultanément
    \item \textbf{Dates valides} : \texttt{order\_date} <= \texttt{ship\_date}
    \item \textbf{Prix cohérents} : Tous les prix sont positifs
    \item \textbf{Inventaire complet} : Toutes les combinaisons store × product présentes
\end{itemize}

\subsection{Exemples de Requêtes de Validation}

\begin{lstlisting}[language=SQL, caption=Vérification de l'intégrité]
-- Verifier qu'aucun produit n'a d'investisseur invalide
SELECT COUNT(*) 
FROM product p
LEFT JOIN investor i ON p.id_investisseur = i.id
WHERE i.id IS NULL;

-- Verifier la hierarchie bidding complete
SELECT COUNT(*) FROM section s
JOIN face f ON s.face_id = f.face_id
JOIN rang r ON f.rang_id = r.rang_id
JOIN category c ON r.category_id = c.category_id;
\end{lstlisting}

\section{Optimisations et Performances}

\subsection{Indexation}

Des index ont été automatiquement créés par PostgreSQL sur :
\begin{itemize}
    \item Toutes les clés primaires
    \item Toutes les clés étrangères
    \item Colonnes fréquemment utilisées dans les jointures
\end{itemize}

\subsection{Taille de la Base de Données}

Taille approximative après injection complète : \textbf{~250 MB}.

\section{Conclusion}

Ce processus de transformation a permis de :
\begin{itemize}
    \item Normaliser un dataset CSV brut en 18 tables relationnelles
    \item Générer plus d'1 million de lignes de données cohérentes
    \item Créer un écosystème complet d'utilisateurs avec rôles différenciés
    \item Implémenter une hiérarchie de bidding fonctionnelle
    \item Garantir l'intégrité référentielle et la qualité des données
\end{itemize}

Cette base de données constitue le fondement du système Analify, permettant des analyses riches et un système de bidding réaliste.
